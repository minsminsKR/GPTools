from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
import streamlit as st
import tempfile
from langchain.callbacks.base import BaseCallbackHandler
import os
import openai

# Streamlit ì›¹ì•±ì—ì„œ Vector DBë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•¨. ë¡œì»¬í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ì‹œ ë¹„í™œì„±í™”.
# This code is for using Vector DB in Streamlit web app. Inactivate it when you want in a local environment.
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
###########################################################################

openai.api_key = st.secrets["api_key"]
# ê¸€ì í•˜ë‚˜ì”© ì‹¤ì‹œê°„ìœ¼ë¡œ ë„ìš°ê¸°
# Printing letters one by one in real time
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text=initial_text
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text+=token
        self.container.markdown(self.text)

def main():
    st.title('ğŸ¦œğŸ”—Welcome to GPTools :scroll:')
    st.sidebar.success("ğŸ‘† Select a page above.")
    
    uploaded_files = st.file_uploader("Upload one or multiple files. Supported formats are **pdf, docx, doc** and **txt**. Your files don't be stored here.", accept_multiple_files=True)
    if uploaded_files:
        text = []
        # Uploaded_fileì„ ê°ì²´ë¡œ ë°˜í™˜. í•˜ì§€ë§Œ ê²½ë¡œë¥¼ ë¬¸ìì—´ì´ë‚˜ ë°”ì´íŠ¸ í˜•íƒœë¡œ ì œê³µí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—
        # PyPDFLoaderì™€ ê°™ì€ ëª‡ëª‡ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„  ì˜¤ë¥˜. ì§ì ‘ì ìœ¼ë¡œ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì´ ì—†ê¸°ë•Œë¬¸ì´ë‹¤.
        # ë”°ë¼ì„œ ì„ì‹œê²½ë¡œë¥¼ ìƒì„±í•˜ê³  í•´ë‹¹ ê²½ë¡œë¥¼ ê°ê°Loaderë“¤ë¡œ ì „ë‹¬í•œë‹¤.
    
        # Returns Uploaded_file as an object, but errors in some libraries, such as PyPDFLoader, because it does not provide paths in string or byte form.
        # This is because there is no function to directly process byte data. Therefore, a temporary file is created and the corresponding path is delivered to the loaders respectively.
    
        for file in uploaded_files:
            file_extension = os.path.splitext(file.name)[1]
            with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                temp_file.write(file.read())
                temp_file_path = temp_file.name
            # loader = None
            if file_extension == ".pdf":
                loader = PyPDFLoader(temp_file_path)
            elif file_extension == ".docx" or file_extension == ".doc":
                loader = Docx2txtLoader(temp_file_path)
            elif file_extension == ".txt":
                loader = TextLoader(temp_file_path)
            if loader:
                text.extend(loader.load())
                os.remove(temp_file_path)
    
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 100)
        texts = text_splitter.split_documents(text)
    
        # Database, Embedding
        with st.spinner("Processing"):
            # persist_directory = 'db' # Save to directory named 'db' (if you want to use in local env)
            persist_directory = tempfile.mkdtemp() # temporary directory
            embedding = OpenAIEmbeddings(openai_api_key=st.secrets["api_key"])
            vectordb = Chroma.from_documents(
                documents=texts,
                embedding = embedding, # Which embedding
                persist_directory = persist_directory)
        
        # Initialization
        vectordb.persist()
        vectordb = None
    
        vectordb = Chroma(
            persist_directory = persist_directory,
            embedding_function = embedding
        )
        retriever = vectordb.as_retriever()
    
        with st.form("form", clear_on_submit=True):
            st.header("Chat with GPT ğŸ˜„")
            question = st.text_input("Ask anything about your files", placeholder="Enter to submit")
            submit = st.form_submit_button("Submit")
            if submit and question:
                with st.spinner("Wait for it..."):
                    chat_box = st.empty()
                    stream_hander = StreamHandler(chat_box)
                    qa_chain = RetrievalQA.from_chain_type(
                        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, streaming=True, callbacks=[stream_hander], openai_api_key=st.secrets["api_key"]),
                        retriever=retriever,
                        return_source_documents = True, 
                        chain_type = "stuff")
                    qa_chain({"query": question})
                    
if __name__ == '__main__' :
    main()
